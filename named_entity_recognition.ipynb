{"cells":[{"cell_type":"markdown","metadata":{"id":"fWeisQHVRdqk"},"source":["<center>\n","<h1 style=\"font-family:verdana\">\n"," üíª üßë Classificaci√≥ d'intencions üßë üíª\n"]},{"cell_type":"markdown","metadata":{"id":"OYVGfnG6MhT8"},"source":["<p> üéØ <b>Objectiu</b>: en aquesta pr√†ctica aprendrem a detectar la intenci√≥ de l'usuari a partir d'interaccions reals amb un xatbot. En el context de xatbots, la classificaci√≥ d'intencions ajuda a entendre quina acci√≥ o resposta hauria de prendre el sistema en funci√≥ de la consulta de l'usuari.  \n","\n","\n","<p> ‚ú® <b>Contingut</b>: en primer lloc, farem servir una base de dades amb oracions d'interaccions en espanyol etiquetades com 19 intencions diferents. En segon lloc, realitzarem el preprocessament de les dades, √©s a dir, transformarem les dades perqu√® tinguen un format adequat per a ser introdu√Ødes al model. I finalment, dissenyarem i entrenarem el model de classificaci√≥ per detectar autom√†ticament la intenci√≥ de les oracions.</p>  \n","\n","\n","<p> ‚úè <b>Exercicis</b>: en cada secci√≥ anireu trobant exercicis que haureu d'anar resolent. </p>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ff3yPMsPihza"},"source":["---\n","\n","<h2> √çndex </h2>\n","\n","1. [Inspecci√≥ del conjunt de dades](#section-one)\n","  * [Exercici 1](#ex-one)\n","2. [Preprocessament de dades](#section-two)\n","  * [Exercici 2](#ex-two)\n","  * [Exercici 3](#ex-three)\n","3. [Disseny del model i entrenament](#section-three)\n","  * [Exercici 4](#ex-four)\n","  * [Exercici 5](#ex-five)\n","4. [Lliurable](#section-four)\n","---"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"KQ5ViCr82quX"},"outputs":[],"source":["import random\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense, GlobalMaxPooling1D, Dropout, Conv1D, GlobalAveragePooling1D, LayerNormalization #Remove\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xcK7bp2ztSCi","outputId":"02dcb68f-a240-4a2e-c3c5-e0e6db343527"},"outputs":[],"source":["!pip install gdown\n","!gdown \"https://drive.google.com/uc?id=1u2wzXvsuscLeFHwXcDwMDaNDy0u_99-t\"\n","!tar -zxf nlu_ATIS_data.tar.gz"]},{"cell_type":"markdown","metadata":{"id":"Q52qMS2g2X7f"},"source":["<h1><a name=\"section-one\"> 1. Inspecci√≥ del conjunt de dades </a></h1>"]},{"cell_type":"markdown","metadata":{"id":"L8P5b9k82vcy"},"source":["A la carpeta `data` tenim els diferents fitxers CSV que utilitzarem per a aquesta pr√†ctica.\n","\n","En primer lloc, llegirem les dades dels fitxers CSV amb `pandas`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":393,"status":"ok","timestamp":1696407048010,"user":{"displayName":"Anna Arias Duart","userId":"05276166712538946860"},"user_tz":-120},"id":"KsBfmAPsnn6n","outputId":"ca09836d-05f1-4be4-f5da-b92d410b8585"},"outputs":[],"source":["train_data = pd.read_csv('./data/train.csv', header=None)\n","val_data = train_data.tail(900)\n","train_data = pd.read_csv('./data/train.csv', header=None, nrows=4078)\n","test_data = pd.read_csv('./data/test.csv', header=None)\n","\n","print('Training size:', len(train_data))\n","print('Validation dataset size:', len(val_data))\n","print('Test dataset size:', len(test_data))"]},{"cell_type":"markdown","metadata":{"id":"zXFrLX9mvdiQ"},"source":["Per a aquesta primera part de la pr√†ctica ens centrarem en la primera columna dels arxius que correspon amb les **oracions** en angl√®s introdu√Ødes per l'usuari. I en la tercera columna que correspon amb la **intenci√≥** de cada oraci√≥, √©s a dir, cada oraci√≥ tindr√† una etiqueta.\n","\n","Podeu executar la cel¬∑la seg√ºent tantes vegades com vulgueu per veure inst√†ncies d'aquest conjunt de dades.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":309,"status":"ok","timestamp":1696407054216,"user":{"displayName":"Anna Arias Duart","userId":"05276166712538946860"},"user_tz":-120},"id":"tJ4svJaAwQXo","outputId":"47f9be19-f65e-4e9e-e64b-d1e851fc9580"},"outputs":[],"source":["random_number = random.randint(0, len(train_data)-1)\n","\n","train_sentences = list(train_data[0])\n","train_labels = list(s.replace('\"', '') for s in train_data[2])\n","train_labels = list(s.replace(' ', '') for s in train_labels)\n","\n","print('Sentence: ', train_sentences[random_number])\n","print('Intent: ', train_labels[random_number])"]},{"cell_type":"markdown","metadata":{"id":"mjhulZcA0aPG"},"source":["A continuaci√≥ analitzarem quantes etiquetes diferents hi ha al dataset i quines s√≥n."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":257,"status":"ok","timestamp":1696407057847,"user":{"displayName":"Anna Arias Duart","userId":"05276166712538946860"},"user_tz":-120},"id":"C9eS7iLazf-i","outputId":"c97a53db-94fb-4143-b17a-1e0964d4ba87"},"outputs":[],"source":["num_labels = 0\n","for label in set(train_labels):\n","  print(f'Label {num_labels}:', label.split('.')[-1])\n","  num_labels += 1\n","\n","print(f'\\nThere are a total of {num_labels} intent labels')"]},{"cell_type":"markdown","metadata":{"id":"hsjk95_B4pXY"},"source":["<h1><a name=\"section-two\"> 2. Preprocessament de dades </a></h1>"]},{"cell_type":"markdown","metadata":{"id":"o-Ita3Yf46R5"},"source":["En primer lloc, haurem de tokenitzar les oracions. Aix√≤ consisteix a convertir el text en representacions num√®riques, ja que els models esperen unitats discretes.\n","\n","En aquesta pr√†ctica farem servir una tokenitzaci√≥ senzilla, simplement dividirem les oracions en paraules i crearem un vocabulari basat en les paraules √∫niques de les dades d'entrenament. Cada paraula (token) tindr√† assignat un ID √∫nic.\n","\n","Vegem com queda el vocabulari."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zbAD9luKn1X3","outputId":"ffe4bd28-075e-499d-d562-384f1da4f382"},"outputs":[],"source":["num_words=500\n","tokenizer = Tokenizer(num_words)\n","tokenizer.fit_on_texts(train_sentences)\n","\n","vocab = tokenizer.word_index\n","print(vocab)"]},{"cell_type":"markdown","metadata":{"id":"6hGwpkozALwY"},"source":["---\n","\n"," <h1><a name=\"ex-one\"><center> ‚úè Exercici 1 ‚úè</a></h1>"]},{"cell_type":"markdown","metadata":{"id":"aUg-yEv3AQHF"},"source":["En aquest primer exercici us demanem que donat el vocabulari anterior convertiu la llista d'oracions de la partici√≥ d'entrenament, √©s a dir, `train_sentenes` en seq√º√®ncies d'IDs.\n","\n","Podeu trobar la documentaci√≥ [aqu√≠](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GKe6R8slB3BY"},"outputs":[],"source":["#TODO\n","#train_sequences = ...\n","#print(train_sentences[0])\n","#print(train_sequences[0])"]},{"cell_type":"markdown","metadata":{"id":"hWD1yHt6B5-v"},"source":["Si ho heu fet correctament haur√≠eu d'obtenir aix√≤:\n","\n","```\n","print(train_sentences[0])\n","print(train_sequences[0])\n","\n","i want to fly from boston at 838 am and arrive in denver at 1110 in the morning\n","[12, 69, 1, 38, 2, 9, 64, 415, 84, 17, 75, 16, 13, 64, 493, 16, 4, 36]\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"o0XsJ2MBDN-C"},"source":["---\n","A continuaci√≥ haurem d'aconseguir que totes les seq√º√®ncies tinguen una longitud fixa. Per a fer aix√≤ primer fixarem la longitud segons la longitud m√†xima trobada a les seq√º√®ncies del conjunt d'entrenament. I a continuaci√≥ omplirem (*pad*) les seq√º√®ncies que tinguen una longitud menor.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pCZCtmemEJcC"},"outputs":[],"source":["max_sequence_length = max(map(len, train_sequences))\n","train_pad_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length)\n","print('Padded sequence: ', train_pad_sequences[0])"]},{"cell_type":"markdown","metadata":{"id":"pra9IsdHF__M"},"source":["---\n","\n"," <h1><a name=\"ex-two\"><center> ‚úè Exercici 2 ‚úè</a></h1>"]},{"cell_type":"markdown","metadata":{"id":"PBgfE1taGCFZ"},"source":["Com l'ordre de les paraules s√≠ que importa als models que utilitzarem en aquesta pr√†ctica, √©s aconsellable que el *padding* estiga al final i no al principi. Busqueu [aqu√≠](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences) com fer perqu√® el codi anterior afegisca els zeros al final i no al principi de la seq√º√®ncia."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VpZd7KQsHIEH"},"outputs":[],"source":["#TODO\n","#train_pad_sequences = pad_sequences ...\n","#print('Padded sequence: ', train_pad_sequences[0])"]},{"cell_type":"markdown","metadata":{"id":"KjuIPHd7HNRY"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"_l6lJA9mHj3j"},"source":["A continuaci√≥ convertirem les classes d'intencions categ√≤riques (*capacity*, *ground_service*, *flight*, etc.) en el que anomenem one-hot vector encoding. Aquesta t√®cnica s'utilitza per representar les dades categ√≤riques com a vectors binaris. On cada vector representa una classe espec√≠fica i l'element corresponent a la classe es posa a 1 i la resta d'elements es mantenen a 0.\n","\n","Imaginem que tenim tres classes: *capacity*, *ground_service*, *flight*. Podr√≠em codificar aquestes classes amb un vector √∫nic de la forma seg√ºent:\n","\n","\n","```\n","   capacity -> [1, 0, 0]\n","   ground_service -> [0, 1, 0]\n","   flight -> [0, 0, 1]\n","```\n","\n","Per aconseguir aix√≤ primer codificarem les classes d'intenci√≥ en etiquetes num√®riques."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j8c6wt1Pb7fw"},"outputs":[],"source":["label_encoder = LabelEncoder()\n","train_numerical_labels = label_encoder.fit_transform(train_labels)\n","\n","print(f'Original labels: {train_labels}\\n')\n","print(f'Encoded labels: {train_numerical_labels} \\n')"]},{"cell_type":"markdown","metadata":{"id":"7Ni3g_AsfSB6"},"source":["I a continuaci√≥ convertim les etiquetes a vectors one-hot."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bu_KEG7hgFKO"},"outputs":[],"source":["num_classes = len(np.unique(train_numerical_labels))\n","train_encoded_labels = to_categorical(train_numerical_labels, num_classes)\n","\n","print('Example: \\n')\n","print(f'Original label: {train_labels[0]}\\n')\n","print(f'Numerical label: {train_numerical_labels[0]}\\n')\n","print(f'One-hot: {train_encoded_labels[0]}\\n')"]},{"cell_type":"markdown","metadata":{"id":"Sup3vzTkNg0q"},"source":["---\n","\n"," <h1><a name=\"ex-three\"><center> ‚úè Exercici 3 ‚úè</a></h1>"]},{"cell_type":"markdown","metadata":{"id":"n3Rpxi7PNjB3"},"source":["Amb la partici√≥ de validaci√≥ i test haurem de realitzar els mateixos passos. Per tant, en aquest exercici us demanem que obtingueu `val_pad_sequences`, `val_encoded_labels`, `test_pad_sequences` i `test_encoded_labels`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8YFVmi4SOPv2"},"outputs":[],"source":["#TODO\n","#val_pad_sequences = ...\n","\n","#test_pad_sequences = ...\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0MIvWt6nre2"},"outputs":[],"source":["val_labels = list(val_data[2])\n","test_labels = list(test_data[2])\n","\n","def remove_values_and_indices(input_list, values_to_remove, other_list):\n","    indices_to_remove = [idx for idx, item in enumerate(input_list) if item in values_to_remove]\n","    cleaned_list = [item for item in input_list if item not in values_to_remove]\n","    cleaned_other_list = [item for idx, item in enumerate(other_list) if idx not in indices_to_remove]\n","    return cleaned_list, np.array(cleaned_other_list)\n","\n","values_to_remove = ['day_name','airfare+flight','flight+airline','flight_no+airline']\n","val_labels, val_pad_sequences = remove_values_and_indices(val_labels, values_to_remove, val_pad_sequences)\n","test_labels, test_pad_sequences = remove_values_and_indices(test_labels, values_to_remove, test_pad_sequences)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDOehXO--rn7"},"outputs":[],"source":["#val_encoded_labels = ...\n","#test_encoded_labels = ..."]},{"cell_type":"markdown","metadata":{"id":"5iS0Z1mbOZcM"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"8Rddmu0bM0PI"},"source":["<h1><a name=\"section-three\"> 3. Disseny del model i entrenament </a></h1>"]},{"cell_type":"markdown","metadata":{"id":"o3mi1pL38yU4"},"source":["En primer lloc, anem a comprovar si hi ha GPUs disponibles. A continuaci√≥ si hi ha GPUs disponibles el codi assegurar√† que *TensorFlow* nom√©s assigne mem√≤ria GPU quan siga necessari."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Dx4NjOc6PYx","outputId":"7244a3ac-3428-43c8-9ef7-771f53462ccd"},"outputs":[],"source":["if tf.config.list_physical_devices('GPU'):\n","    print(\"GPU is available!\")\n","else:\n","    print(\"GPU is not available. The model will be trained on CPU.\")\n","\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","    except RuntimeError as e:\n","        print(e)"]},{"cell_type":"markdown","metadata":{"id":"0CPFNkeCKSVl"},"source":["---\n","\n"," <h1><a name=\"ex-four\"><center> ‚úè Exercici 4 ‚úè</a></h1>"]},{"cell_type":"markdown","metadata":{"id":"woZEkupw0LAB"},"source":["En aquest exercici haureu de dissenyar l'arquitectura del model. El nostre model tindr√† quatre capes:\n","\n","1. La primera capa ser√† un **embedding**. Aquesta capa permetr√† convertir les dades de text d'entrada, en vectors densos amb una mida fixa (*embedding_dim*). Aquesta representaci√≥ m√©s compacta permetr√† per una part capturar la informaci√≥ sem√†ntica del text d'entrada, permetent aix√≠ generalitzar millor i comprendre les relacions entre les paraules. I, per una altra banda, reduir la complexitat computacional, accelerant aix√≠ el temps d'entrenament i infer√®ncia. En resum, aquesta capa assignar√† a cada √≠ndex de cada paraula un vector dens de mida *embedding_dim*.\n","\n","2. La segona capa ser√† un **pooling** layer. L'entrada d'aquesta capa ser√† un tensor 3D (*batch_size*, *sequence_length*, *embedding_dim*). Aquesta capa es centrar√† a capturar la informaci√≥ m√©s important de la seq√º√®ncia d'entrada, √©s a dir, prendr√† el valor m√†xim de la seq√º√®ncia, donant lloc a un tensor 2D (batch_size, embedding_dim).\n","\n","3. La tercera capa ser√† una capa **densa**. √âs a dir, una capa completament connectada (*fully-connected*): cada neurona d'aquesta capa estar√† connectada a totes les neurones de la capa anterior. La funci√≥ d'activaci√≥ que utilitzarem ser√† una ReLU. Aquesta funci√≥ introdueix una no-linealitat al model permetent aix√≠ aprendre relacions complexes en les dades.\n","\n","4. L'√∫ltima capa tamb√© ser√† una capa **densa**. En aquest cas la funci√≥ d'activaci√≥ haur√† de ser la funci√≥ Softmax. Aquesta funci√≥ es fa servir per a convertir els valors de la capa anterior (*logits*) en probabilitats normalitzades. El valor de cada element de sortida representar√† la probabilitat que l'entrada pertanya a una classe espec√≠fica.\n","\n","\n","üì¢  Les capes que haureu de fer servir les podreu trobar [aqu√≠](https://www.tensorflow.org/api_docs/python/tf/keras/layers).\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vUv4tsRAJh8o"},"outputs":[],"source":["#TODO\n","#embedding_dim = ...\n","'''\n","vocab_size = num_words + 1\n","\n","model = Sequential()\n","model.add() #layer 1\n","model.add() #layer 2\n","model.add() #layer 3\n","model.add() #layer 4\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","#batch_size = 32\n","#epochs = 2\n","model.fit(train_pad_sequences, train_encoded_labels, batch_size=batch_size, epochs=epochs, validation_data=(val_pad_sequences, val_encoded_labels))\n","\n","# Evaluate the model on the test set\n","loss, accuracy = model.evaluate(test_pad_sequences, test_encoded_labels, batch_size=batch_size)\n","print(f\"Test accuracy: {accuracy:.2f}\")\n","'''"]},{"cell_type":"markdown","metadata":{"id":"exbeyhxdVC1M"},"source":["Podeu veure a continuaci√≥ les oracions que el model ha classificat incorrectament."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0JErcq5OK7e"},"outputs":[],"source":["probs = model.predict(test_pad_sequences)\n","_predicted_labels = np.argmax(probs, axis=1)\n","predicted_labels = label_encoder.inverse_transform(_predicted_labels)\n","\n","for i in range(0, len(predicted_labels)):\n","  if test_labels[i] != predicted_labels[i]:\n","    print('Sentence: ', test_sentences[i])\n","    print('Original label: ', test_labels[i])\n","    print('Predicted label: ', predicted_labels[i])\n","    print()\n"]},{"cell_type":"markdown","metadata":{"id":"vgKO2p_jSfh6"},"source":["---\n","\n"," <h1><a name=\"ex-five\"><center> ‚úè Exercici 5 ‚úè </a></h1>\n","\n","Modifiqueu els seg√ºents par√†metres del model anterior i analitzeu com afecten a la seva *accuracy*:\n","\n"," 1. **Preprocessament.** Modifiqueu el Tokenizer per canviar la mida del vostre vocabulari i afegiu nous passos de preprocessament. Alguns possibles canvis s√≥n canviar la mida del vocabulari, treure la capitalitzaci√≥ o fer servir *lemmatitzaci√≥* o *stemming*.\n","\n"," 2. **Mida dels Embeddings.** Proveu diferents mides d'*Embeddings* i observeu com canvia l'*accuracy* del model. Heu d'explicar les vostres conclusions.\n","\n"," 3. **Xarxes Convolucionals.** Afegiu capes convolucionals al vostre model. Expliqueu amb detall els valors que heu provat i la vostra motivaci√≥ a l'hora d'escollir-los. Recordeu, que tamb√© podeu provar diferents configuracions de *pooling*.\n","\n"," 4. **Xarxes Recurrents.**  Afegiu capes recurrents al vostre model (LSTM, GRU). Expliqueu amb detall els valors que heu provat i la vostra motivaci√≥.\n","\n"," 5. **Regularitzaci√≥.** Quan proveu configuracions amb m√©s par√†metres veureu que el model comen√ßa a tenir *overfitting* molt prompte durant l'entrenament. Afegiu *Dropout* al vostre model. Heu d'explicar la vostra decisi√≥ de valors i de posici√≥ dins de la xarxa.\n","\n"," 6. **Balancejat de les classes.** Si analitzeu el dataset, veureu que la freq√º√®ncia de les classes est√† molt desbalancejada. Keras us permet afegir un pes per a cada classe a l'hora de calcular la loss (Mireu el par√†metre \"class_weigth\" a la documentaci√≥ https://keras.io/api/models/model_training_apis/). Calculeu un pes per a cada classe i afegiu-lo al m√®tode fit del vostre model.\n","\n"," ---"]},{"cell_type":"markdown","metadata":{"id":"oJL2VFSu6sce"},"source":["\n","<h1><a name=\"section-four\"> 4. Lliurable </a></h1>\n","\n","Heu d'entregar un document PDF de com a **m√†xim 10 p√†gines** que incloga els resultats de tots els exercicis aix√≠ com una explicaci√≥ de cadascun dels resultats i de la modificaci√≥ que heu fet. L'estructura del document √©s:\n","\n","1. Introducci√≥.\n","2. Experiments i Resultats (amb raonament).\n","3. Conclusions.\n","\n","No cal que afegiu el vostre codi al document, podeu entregar el *notebook* juntament amb el document.\n","\n"," ---"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1Hzv2UnnX9qt2tfevnrW9wKKyxz49enYa","timestamp":1695718475771}]},"kernelspec":{"display_name":"TVD","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
