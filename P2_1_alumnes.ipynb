{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWeisQHVRdqk"
   },
   "source": [
    "<center>\n",
    "<h1 style=\"font-family:verdana\">\n",
    " üíª üßë Classificaci√≥ d'intencions üßë üíª\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYVGfnG6MhT8"
   },
   "source": [
    "<p> üéØ <b>Objectiu</b>: en aquesta pr√†ctica aprendrem a detectar la intenci√≥ de l'usuari a partir d'interaccions reals amb un xatbot. En el context de xatbots, la classificaci√≥ d'intencions ajuda a entendre quina acci√≥ o resposta hauria de prendre el sistema en funci√≥ de la consulta de l'usuari.  \n",
    "\n",
    "\n",
    "<p> ‚ú® <b>Contingut</b>: en primer lloc, farem servir una base de dades amb oracions d'interaccions en espanyol etiquetades com 19 intencions diferents. En segon lloc, realitzarem el preprocessament de les dades, √©s a dir, transformarem les dades perqu√® tinguen un format adequat per a ser introdu√Ødes al model. I finalment, dissenyarem i entrenarem el model de classificaci√≥ per detectar autom√†ticament la intenci√≥ de les oracions.</p>  \n",
    "\n",
    "\n",
    "<p> ‚úè <b>Exercicis</b>: en cada secci√≥ anireu trobant exercicis que haureu d'anar resolent. </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff3yPMsPihza"
   },
   "source": [
    "---\n",
    "\n",
    "<h2> √çndex </h2>\n",
    "\n",
    "1. [Inspecci√≥ del conjunt de dades](#section-one)\n",
    "  * [Exercici 1](#ex-one)\n",
    "2. [Preprocessament de dades](#section-two)\n",
    "  * [Exercici 2](#ex-two)\n",
    "  * [Exercici 3](#ex-three)\n",
    "3. [Disseny del model i entrenament](#section-three)\n",
    "  * [Exercici 4](#ex-four)\n",
    "  * [Exercici 5](#ex-five)\n",
    "4. [Lliurable](#section-four)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQ5ViCr82quX"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, GlobalMaxPooling1D, Dropout, Conv1D, GlobalAveragePooling1D, LayerNormalization,Bidirectional #Remove\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time  \n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet,stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcK7bp2ztSCi",
    "outputId": "02dcb68f-a240-4a2e-c3c5-e0e6db343527"
   },
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!gdown \"https://drive.google.com/uc?id=1u2wzXvsuscLeFHwXcDwMDaNDy0u_99-t\"\n",
    "!tar -zxf nlu_ATIS_data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q52qMS2g2X7f"
   },
   "source": [
    "<h1><a name=\"section-one\"> 1. Inspecci√≥ del conjunt de dades </a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8P5b9k82vcy"
   },
   "source": [
    "A la carpeta `data` tenim els diferents fitxers CSV que utilitzarem per a aquesta pr√†ctica.\n",
    "\n",
    "En primer lloc, llegirem les dades dels fitxers CSV amb `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1696407048010,
     "user": {
      "displayName": "Anna Arias Duart",
      "userId": "05276166712538946860"
     },
     "user_tz": -120
    },
    "id": "KsBfmAPsnn6n",
    "outputId": "ca09836d-05f1-4be4-f5da-b92d410b8585"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.csv', header=None)\n",
    "val_data = train_data.tail(900)\n",
    "train_data = pd.read_csv('./data/train.csv', header=None, nrows=4078)\n",
    "test_data = pd.read_csv('./data/test.csv', header=None)\n",
    "\n",
    "print('Training size:', len(train_data))\n",
    "print('Validation dataset size:', len(val_data))\n",
    "print('Test dataset size:', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXFrLX9mvdiQ"
   },
   "source": [
    "Per a aquesta primera part de la pr√†ctica ens centrarem en la primera columna dels arxius que correspon amb les **oracions** en angl√®s introdu√Ødes per l'usuari. I en la tercera columna que correspon amb la **intenci√≥** de cada oraci√≥, √©s a dir, cada oraci√≥ tindr√† una etiqueta.\n",
    "\n",
    "Podeu executar la cel¬∑la seg√ºent tantes vegades com vulgueu per veure inst√†ncies d'aquest conjunt de dades.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1696407054216,
     "user": {
      "displayName": "Anna Arias Duart",
      "userId": "05276166712538946860"
     },
     "user_tz": -120
    },
    "id": "tJ4svJaAwQXo",
    "outputId": "47f9be19-f65e-4e9e-e64b-d1e851fc9580"
   },
   "outputs": [],
   "source": [
    "random_number = random.randint(0, len(train_data)-1)\n",
    "\n",
    "train_sentences = list(train_data[0])\n",
    "train_labels = list(s.replace('\"', '') for s in train_data[2])\n",
    "train_labels = list(s.replace(' ', '') for s in train_labels)\n",
    "\n",
    "print('Sentence: ', train_sentences[random_number])\n",
    "print('Intent: ', train_labels[random_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjhulZcA0aPG"
   },
   "source": [
    "A continuaci√≥ analitzarem quantes etiquetes diferents hi ha al dataset i quines s√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1696407057847,
     "user": {
      "displayName": "Anna Arias Duart",
      "userId": "05276166712538946860"
     },
     "user_tz": -120
    },
    "id": "C9eS7iLazf-i",
    "outputId": "c97a53db-94fb-4143-b17a-1e0964d4ba87"
   },
   "outputs": [],
   "source": [
    "num_labels = 0\n",
    "for label in set(train_labels):\n",
    "  print(f'Label {num_labels}:', label.split('.')[-1])\n",
    "  num_labels += 1\n",
    "\n",
    "print(f'\\nThere are a total of {num_labels} intent labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsjk95_B4pXY"
   },
   "source": [
    "<h1><a name=\"section-two\"> 2. Preprocessament de dades </a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-Ita3Yf46R5"
   },
   "source": [
    "En primer lloc, haurem de tokenitzar les oracions. Aix√≤ consisteix a convertir el text en representacions num√®riques, ja que els models esperen unitats discretes.\n",
    "\n",
    "En aquesta pr√†ctica farem servir una tokenitzaci√≥ senzilla, simplement dividirem les oracions en paraules i crearem un vocabulari basat en les paraules √∫niques de les dades d'entrenament. Cada paraula (token) tindr√† assignat un ID √∫nic.\n",
    "\n",
    "Vegem com queda el vocabulari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbAD9luKn1X3",
    "outputId": "ffe4bd28-075e-499d-d562-384f1da4f382"
   },
   "outputs": [],
   "source": [
    "num_words=500\n",
    "tokenizer = Tokenizer(num_words)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "vocab = tokenizer.word_index\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hGwpkozALwY"
   },
   "source": [
    "---\n",
    "\n",
    " <h1><a name=\"ex-one\"><center> ‚úè Exercici 1 ‚úè</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUg-yEv3AQHF"
   },
   "source": [
    "En aquest primer exercici us demanem que donat el vocabulari anterior convertiu la llista d'oracions de la partici√≥ d'entrenament, √©s a dir, `train_sentenes` en seq√º√®ncies d'IDs.\n",
    "\n",
    "Podeu trobar la documentaci√≥ [aqu√≠](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKe6R8slB3BY"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "\n",
    "print(train_sentences[0])\n",
    "print(train_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWD1yHt6B5-v"
   },
   "source": [
    "Si ho heu fet correctament haur√≠eu d'obtenir aix√≤:\n",
    "\n",
    "```\n",
    "print(train_sentences[0])\n",
    "print(train_sequences[0])\n",
    "\n",
    "i want to fly from boston at 838 am and arrive in denver at 1110 in the morning\n",
    "[12, 69, 1, 38, 2, 9, 64, 415, 84, 17, 75, 16, 13, 64, 493, 16, 4, 36]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0XsJ2MBDN-C"
   },
   "source": [
    "---\n",
    "A continuaci√≥ haurem d'aconseguir que totes les seq√º√®ncies tinguen una longitud fixa. Per a fer aix√≤ primer fixarem la longitud segons la longitud m√†xima trobada a les seq√º√®ncies del conjunt d'entrenament. I a continuaci√≥ omplirem (*pad*) les seq√º√®ncies que tinguen una longitud menor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCZCtmemEJcC"
   },
   "outputs": [],
   "source": [
    "max_sequence_length = max(map(len, train_sequences))\n",
    "train_pad_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "print('Padded sequence: ', train_pad_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pra9IsdHF__M"
   },
   "source": [
    "---\n",
    "\n",
    " <h1><a name=\"ex-two\"><center> ‚úè Exercici 2 ‚úè</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBgfE1taGCFZ"
   },
   "source": [
    "Com l'ordre de les paraules s√≠ que importa als models que utilitzarem en aquesta pr√†ctica, √©s aconsellable que el *padding* estiga al final i no al principi. Busqueu [aqu√≠](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences) com fer perqu√® el codi anterior afegisca els zeros al final i no al principi de la seq√º√®ncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpZd7KQsHIEH"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "train_pad_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "print('Padded sequence: ', train_pad_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjuIPHd7HNRY"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_l6lJA9mHj3j"
   },
   "source": [
    "A continuaci√≥ convertirem les classes d'intencions categ√≤riques (*capacity*, *ground_service*, *flight*, etc.) en el que anomenem one-hot vector encoding. Aquesta t√®cnica s'utilitza per representar les dades categ√≤riques com a vectors binaris. On cada vector representa una classe espec√≠fica i l'element corresponent a la classe es posa a 1 i la resta d'elements es mantenen a 0.\n",
    "\n",
    "Imaginem que tenim tres classes: *capacity*, *ground_service*, *flight*. Podr√≠em codificar aquestes classes amb un vector √∫nic de la forma seg√ºent:\n",
    "\n",
    "\n",
    "```\n",
    "   capacity -> [1, 0, 0]\n",
    "   ground_service -> [0, 1, 0]\n",
    "   flight -> [0, 0, 1]\n",
    "```\n",
    "\n",
    "Per aconseguir aix√≤ primer codificarem les classes d'intenci√≥ en etiquetes num√®riques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8c6wt1Pb7fw"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_numerical_labels = label_encoder.fit_transform(train_labels)\n",
    "\n",
    "print(f'Original labels: {train_labels}\\n')\n",
    "print(f'Encoded labels: {train_numerical_labels} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ni3g_AsfSB6"
   },
   "source": [
    "I a continuaci√≥ convertim les etiquetes a vectors one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bu_KEG7hgFKO"
   },
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(train_numerical_labels))\n",
    "train_encoded_labels = to_categorical(train_numerical_labels, num_classes)\n",
    "\n",
    "print('Example: \\n')\n",
    "print(f'Original label: {train_labels[0]}\\n')\n",
    "print(f'Numerical label: {train_numerical_labels[0]}\\n')\n",
    "print(f'One-hot: {train_encoded_labels[0]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sup3vzTkNg0q"
   },
   "source": [
    "---\n",
    "\n",
    " <h1><a name=\"ex-three\"><center> ‚úè Exercici 3 ‚úè</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3Rpxi7PNjB3"
   },
   "source": [
    "Amb la partici√≥ de validaci√≥ i test haurem de realitzar els mateixos passos. Per tant, en aquest exercici us demanem que obtingueu `val_pad_sequences`, `val_encoded_labels`, `test_pad_sequences` i `test_encoded_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sentences, tokenizer, max_sequence_length):\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(labels, label_encoder):\n",
    "\n",
    "    numerical_labels = label_encoder.transform(labels)\n",
    "    one_hot_labels = to_categorical(numerical_labels, num_classes)\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YFVmi4SOPv2"
   },
   "outputs": [],
   "source": [
    "\n",
    "val_pad_sequences = preprocess_sentences(val_data[0], tokenizer, max_sequence_length)\n",
    "test_pad_sequences = preprocess_sentences(test_data[0], tokenizer, max_sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0MIvWt6nre2"
   },
   "outputs": [],
   "source": [
    "val_labels = list(s.replace('\"', '') for s in val_data[2])\n",
    "val_labels = list(s.replace(' ', '') for s in val_labels)\n",
    "\n",
    "test_labels = list(s.replace('\"', '') for s in test_data[2])\n",
    "test_labels = list(s.replace(' ', '') for s in test_labels)\n",
    "\n",
    "def remove_values_and_indices(input_list, values_to_remove, other_list):\n",
    "    indices_to_remove = [idx for idx, item in enumerate(input_list) if item in values_to_remove]\n",
    "    cleaned_list = [item for item in input_list if item not in values_to_remove]\n",
    "    cleaned_other_list = [item for idx, item in enumerate(other_list) if idx not in indices_to_remove]\n",
    "    return cleaned_list, np.array(cleaned_other_list)\n",
    "\n",
    "values_to_remove = ['day_name','airfare+flight','flight+airline','flight_no+airline']\n",
    "val_labels, val_pad_sequences = remove_values_and_indices(val_labels, values_to_remove, val_pad_sequences)\n",
    "test_labels, test_pad_sequences = remove_values_and_indices(test_labels, values_to_remove, test_pad_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDOehXO--rn7"
   },
   "outputs": [],
   "source": [
    "val_encoded_labels = preprocess_labels(val_labels, label_encoder)\n",
    "test_encoded_labels = preprocess_labels(test_labels, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iS0Z1mbOZcM"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Rddmu0bM0PI"
   },
   "source": [
    "<h1><a name=\"section-three\"> 3. Disseny del model i entrenament </a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3mi1pL38yU4"
   },
   "source": [
    "En primer lloc, anem a comprovar si hi ha GPUs disponibles. A continuaci√≥ si hi ha GPUs disponibles el codi assegurar√† que *TensorFlow* nom√©s assigne mem√≤ria GPU quan siga necessari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Dx4NjOc6PYx",
    "outputId": "7244a3ac-3428-43c8-9ef7-771f53462ccd"
   },
   "outputs": [],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available. The model will be trained on CPU.\")\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CPFNkeCKSVl"
   },
   "source": [
    "---\n",
    "\n",
    " <h1><a name=\"ex-four\"><center> ‚úè Exercici 4 ‚úè</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woZEkupw0LAB"
   },
   "source": [
    "En aquest exercici haureu de dissenyar l'arquitectura del model. El nostre model tindr√† quatre capes:\n",
    "\n",
    "1. La primera capa ser√† un **embedding**. Aquesta capa permetr√† convertir les dades de text d'entrada, en vectors densos amb una mida fixa (*embedding_dim*). Aquesta representaci√≥ m√©s compacta permetr√† per una part capturar la informaci√≥ sem√†ntica del text d'entrada, permetent aix√≠ generalitzar millor i comprendre les relacions entre les paraules. I, per una altra banda, reduir la complexitat computacional, accelerant aix√≠ el temps d'entrenament i infer√®ncia. En resum, aquesta capa assignar√† a cada √≠ndex de cada paraula un vector dens de mida *embedding_dim*.\n",
    "\n",
    "2. La segona capa ser√† un **pooling** layer. L'entrada d'aquesta capa ser√† un tensor 3D (*batch_size*, *sequence_length*, *embedding_dim*). Aquesta capa es centrar√† a capturar la informaci√≥ m√©s important de la seq√º√®ncia d'entrada, √©s a dir, prendr√† el valor m√†xim de la seq√º√®ncia, donant lloc a un tensor 2D (batch_size, embedding_dim).\n",
    "\n",
    "3. La tercera capa ser√† una capa **densa**. √âs a dir, una capa completament connectada (*fully-connected*): cada neurona d'aquesta capa estar√† connectada a totes les neurones de la capa anterior. La funci√≥ d'activaci√≥ que utilitzarem ser√† una ReLU. Aquesta funci√≥ introdueix una no-linealitat al model permetent aix√≠ aprendre relacions complexes en les dades.\n",
    "\n",
    "4. L'√∫ltima capa tamb√© ser√† una capa **densa**. En aquest cas la funci√≥ d'activaci√≥ haur√† de ser la funci√≥ Softmax. Aquesta funci√≥ es fa servir per a convertir els valors de la capa anterior (*logits*) en probabilitats normalitzades. El valor de cada element de sortida representar√† la probabilitat que l'entrada pertanya a una classe espec√≠fica.\n",
    "\n",
    "\n",
    "üì¢  Les capes que haureu de fer servir les podreu trobar [aqu√≠](https://www.tensorflow.org/api_docs/python/tf/keras/layers).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = list(test_data[0])\n",
    "\n",
    "# funcion para listar las predicciones incorrectas\n",
    "def incorrect_predictions(model, test_pad_sequences, test_labels, test_sentences):\n",
    "    probs = model.predict(test_pad_sequences)\n",
    "    _predicted_labels = np.argmax(probs, axis=1)\n",
    "    predicted_labels = label_encoder.inverse_transform(_predicted_labels)\n",
    "\n",
    "    for i in range(0, len(predicted_labels)):\n",
    "        if test_labels[i] != predicted_labels[i]:\n",
    "            print('Sentence: ', test_sentences[i])\n",
    "            print('Original label: ', test_labels[i])\n",
    "            print('Predicted label: ', predicted_labels[i])\n",
    "            print()\n",
    "\n",
    "    return None\n",
    "\n",
    "# funcion para graficar el accuracy\n",
    "def plot_accuracy(history):\n",
    "    \"\"\"\n",
    "    Funci√≥n para graficar el accuracy de entrenamiento y validaci√≥n.\n",
    "    \n",
    "    Argumentos:\n",
    "    - history: El historial del entrenamiento devuelto por 'model.fit()'.\n",
    "    \"\"\"\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.lineplot(x=epochs_range, y=acc, label='Training Accuracy')\n",
    "    sns.lineplot(x=epochs_range, y=val_acc, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# funcion para graficar el loss\n",
    "def plot_loss(history):\n",
    "    \"\"\"\n",
    "    Funci√≥n para graficar el loss de entrenamiento y validaci√≥n.\n",
    "    \n",
    "    Argumentos:\n",
    "    - history: El historial del entrenamiento devuelto por 'model.fit()'.\n",
    "    \"\"\"\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.lineplot(x=epochs_range, y=loss, label='Training Loss')\n",
    "    sns.lineplot(x=epochs_range, y=val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUv4tsRAJh8o"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = num_words + 1 # 0 is reserved for unknown words\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding( vocab_size,embedding_dim)) #layer 1\n",
    "model.add(GlobalAveragePooling1D()) #layer 2\n",
    "model.add(Dense(128, activation='relu')) #layer 3\n",
    "model.add(Dense(22,activation='softmax')) #layer 4\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "history = model.fit(train_pad_sequences, train_encoded_labels, batch_size=batch_size, epochs=epochs, validation_data=(val_pad_sequences, val_encoded_labels))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(test_pad_sequences, test_encoded_labels, batch_size=batch_size)\n",
    "\n",
    "plot_accuracy(history)\n",
    "plot_loss(history)\n",
    "\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exbeyhxdVC1M"
   },
   "source": [
    "Podeu veure a continuaci√≥ les oracions que el model ha classificat incorrectament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0JErcq5OK7e"
   },
   "outputs": [],
   "source": [
    "test_sentences = list(test_data[0])\n",
    "\n",
    "probs = model.predict(test_pad_sequences)\n",
    "_predicted_labels = np.argmax(probs, axis=1)\n",
    "predicted_labels = label_encoder.inverse_transform(_predicted_labels)\n",
    "\n",
    "for i in range(0, len(predicted_labels)):\n",
    "  if test_labels[i] != predicted_labels[i]:\n",
    "    print('Sentence: ', test_sentences[i])\n",
    "    print('Original label: ', test_labels[i])\n",
    "    print('Predicted label: ', predicted_labels[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgKO2p_jSfh6"
   },
   "source": [
    "---\n",
    "\n",
    " <h1><a name=\"ex-five\"><center> ‚úè Exercici 5 ‚úè </a></h1>\n",
    "\n",
    "Modifiqueu els seg√ºents par√†metres del model anterior i analitzeu com afecten a la seva *accuracy*:\n",
    "\n",
    " 1. **Preprocessament.** Modifiqueu el Tokenizer per canviar la mida del vostre vocabulari i afegiu nous passos de preprocessament. Alguns possibles canvis s√≥n canviar la mida del vocabulari, treure la capitalitzaci√≥ o fer servir *lemmatitzaci√≥* o *stemming*.\n",
    "\n",
    " 2. **Mida dels Embeddings.** Proveu diferents mides d'*Embeddings* i observeu com canvia l'*accuracy* del model. Heu d'explicar les vostres conclusions.\n",
    "\n",
    " 3. **Xarxes Convolucionals.** Afegiu capes convolucionals al vostre model. Expliqueu amb detall els valors que heu provat i la vostra motivaci√≥ a l'hora d'escollir-los. Recordeu, que tamb√© podeu provar diferents configuracions de *pooling*.\n",
    "\n",
    " 4. **Xarxes Recurrents.**  Afegiu capes recurrents al vostre model (LSTM, GRU). Expliqueu amb detall els valors que heu provat i la vostra motivaci√≥.\n",
    "\n",
    " 5. **Regularitzaci√≥.** Quan proveu configuracions amb m√©s par√†metres veureu que el model comen√ßa a tenir *overfitting* molt prompte durant l'entrenament. Afegiu *Dropout* al vostre model. Heu d'explicar la vostra decisi√≥ de valors i de posici√≥ dins de la xarxa.\n",
    "\n",
    " 6. **Balancejat de les classes.** Si analitzeu el dataset, veureu que la freq√º√®ncia de les classes est√† molt desbalancejada. Keras us permet afegir un pes per a cada classe a l'hora de calcular la loss (Mireu el par√†metre \"class_weigth\" a la documentaci√≥ https://keras.io/api/models/model_training_apis/). Calculeu un pes per a cada classe i afegiu-lo al m√®tode fit del vostre model.\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardarem els millors resultats de cada experiment en un pandas dataframe\n",
    "results = pd.DataFrame(columns=['Experiment', 'Train Accuracy', 'Validation Accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: canvi de preprocessament"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funci√≥ de preproc√©s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "farem una funci√≥ senzilla per no repetir codi, on ens dedicarem a fer flags per els preprocessos que volem. Farem els seg√ºents canvis en el preproc√©s:\n",
    "\n",
    "- canviar la mida del vocabulari \n",
    "- treure la capitalitzaci√≥\n",
    "- fer servir *lemmatitzaci√≥* o *stemming*.\n",
    "- treure stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(train_sentences,sentences,vocab_size,lemmatize,not_capitalize,stop_words):\n",
    "\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        train_sentences = [' '.join([lemmatizer.lemmatize(word) for word in sentence.split()]) for sentence in train_sentences]\n",
    "        sentences = [' '.join([lemmatizer.lemmatize(word) for word in sentence.split()]) for sentence in sentences]\n",
    "    \n",
    "    if not_capitalize:\n",
    "        train_sentences = [sentence.lower() for sentence in train_sentences]\n",
    "        sentences = [sentence.lower() for sentence in sentences]\n",
    "    \n",
    "    if stop_words:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        train_sentences = [' '.join([word for word in sentence.split() if word not in stop_words]) for sentence in train_sentences]\n",
    "        sentences = [' '.join([word for word in sentence.split() if word not in stop_words]) for sentence in sentences]\n",
    "    \n",
    "    if vocab_size == 'all':\n",
    "        tokenizer = Tokenizer()\n",
    "    else:\n",
    "        tokenizer = Tokenizer(num_words=vocab_size)\n",
    "\n",
    "    tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "    max_sequence_length = max(map(len, train_sequences))\n",
    "    \n",
    "    padded_sequences = preprocess_sentences(sentences, tokenizer, max_sequence_length)\n",
    "    \n",
    "    if vocab_size == 'all':\n",
    "        return padded_sequences,len(tokenizer.word_index) + 1\n",
    "    else:\n",
    "        return padded_sequences,vocab_size\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara guardem els resultats de accuracy de train i validaci√≥ donat les combinacions de preproc√©s que s√≥n possibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudi del Preproc√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = list(train_data[0])\n",
    "val_sentences = list(val_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "vocab_size = [100, 500, 'all']\n",
    "lemmatize = [True, False]\n",
    "not_capitalize = [True, False]\n",
    "stop_words = [True, False]\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for v in vocab_size:\n",
    "    for l in lemmatize:\n",
    "        for c in not_capitalize:\n",
    "            for s in stop_words:\n",
    "                print(f'Vocab size: {v}, Lemmatize: {l}, Not capitalize: {c}, Stop words: {s}')\n",
    "                \n",
    "                train_pad_sequences,vocab_size = preprocessing(train_sentences, train_sentences, v, l, c, s)\n",
    "                val_pad_sequences,_ = preprocessing(train_sentences, val_sentences, v, l, c, s)\n",
    "\n",
    "                model = Sequential()\n",
    "\n",
    "                model.add(Embedding( vocab_size,embedding_dim)) #layer 1\n",
    "                model.add(GlobalAveragePooling1D()) #layer 2\n",
    "                model.add(Dense(128, activation='relu')) #layer 3\n",
    "                model.add(Dense(22,activation='softmax')) #layer 4\n",
    "\n",
    "                # Compile the model\n",
    "                model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                     \n",
    "                history = model.fit(train_pad_sequences, train_encoded_labels, \n",
    "                                    batch_size=batch_size, epochs=epochs, \n",
    "                                    validation_data=(val_pad_sequences, val_encoded_labels))\n",
    "                \n",
    "                resultados.append({\n",
    "                    'Vocab Size': v,\n",
    "                    'Lemmatize': l,\n",
    "                    'Not Capitalize': c,\n",
    "                    'Stop Words': s,\n",
    "                    'Train Accuracy': history.history['accuracy'][-1],\n",
    "                    'Validation Accuracy': history.history['val_accuracy'][-1]\n",
    "                })\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Millors resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultados.sort_values(by='Validation Accuracy', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pitjors resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultados.sort_values(by='Validation Accuracy', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardem a results dataframe els millor resultat de validaci√≥ del experiment 1\n",
    "millors_resultats = df_resultados.sort_values(by='Validation Accuracy', ascending=False).head(1)\n",
    "\n",
    "result_instance = pd.DataFrame({\n",
    "    'Experiment': ['Experiment 1'], \n",
    "    'Train Accuracy': [millors_resultats['Train Accuracy'].values[0]], \n",
    "    'Validation Accuracy': [millors_resultats['Validation Accuracy'].values[0]]\n",
    "})\n",
    "\n",
    "results = pd.concat([results, result_instance], ignore_index=True)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Mida dels embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Millor preprocessament del experiment anterior\n",
    "best_param = millors_resultats[['Vocab Size', 'Lemmatize', 'Not Capitalize', 'Stop Words']].values[0]\n",
    "print(best_param)\n",
    "\n",
    "train_pad_sequences,vocab_size = preprocessing(train_sentences, train_sentences, best_param[0], best_param[1], best_param[2], best_param[3])\n",
    "val_pad_sequences,_ = preprocessing(train_sentences, val_sentences, best_param[0], best_param[1], best_param[2], best_param[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = [50, 100, 200, 400, 800, 1600, 3200, 6400]\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for e in embedding_dim:\n",
    "\n",
    "    print(f'Embedding dim: {e}')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, e))  # layer 1\n",
    "    model.add(GlobalAveragePooling1D())  # layer 2\n",
    "    model.add(Dense(128, activation='relu'))  # layer 3\n",
    "    model.add(Dense(22, activation='softmax'))  # layer 4\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    history = model.fit(train_pad_sequences, train_encoded_labels, \n",
    "                        batch_size=batch_size, epochs=epochs, \n",
    "                        validation_data=(val_pad_sequences, val_encoded_labels))\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    training_time = end_time - start_time\n",
    "    resultados.append({\n",
    "        'Embedding Dim': e,\n",
    "        'Train Accuracy': history.history['accuracy'][-1],\n",
    "        'Validation Accuracy': history.history['val_accuracy'][-1],\n",
    "        'Training Time (seconds)': training_time,\n",
    "        'Validation Accuracy / Training Time': history.history['val_accuracy'][-1] / training_time\n",
    "    })\n",
    "\n",
    "df_embeddings = pd.DataFrame(resultados)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embeddings.sort_values(by='Validation Accuracy / Training Time', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardem a results els millor resultat de validaci√≥ del experiment 2\n",
    "millors_resultats = df_embeddings.sort_values(by='Validation Accuracy / Training Time', ascending=False).head(1)\n",
    "\n",
    "result_instance = pd.DataFrame({\n",
    "    'Experiment': ['Experiment 2'], \n",
    "    'Train Accuracy': [millors_resultats['Train Accuracy'].values[0]], \n",
    "    'Validation Accuracy': [millors_resultats['Validation Accuracy'].values[0]]\n",
    "})\n",
    "\n",
    "results = pd.concat([results, result_instance], ignore_index=True)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Xarxes Convolucionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Millors par√†metres dels experiments anteriors\n",
    "best_param = millors_resultats[['Embedding Dim']].values[0]\n",
    "print(best_param)\n",
    "\n",
    "train_pad_sequences,vocab_size = preprocessing(train_sentences, train_sentences, 'all', True, True, False)\n",
    "val_pad_sequences,_ = preprocessing(train_sentences, val_sentences, 'all', True, True, False)\n",
    "embedding_dim = best_param[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding( vocab_size,embedding_dim)) #layer 1\n",
    "\n",
    "#convulutional layer\n",
    "model.add(Conv1D(10, 5, activation='relu')) \n",
    "\n",
    "model.add(GlobalAveragePooling1D()) #layer 2\n",
    "model.add(Dense(128, activation='relu')) #layer 3\n",
    "model.add(Dense(22,activation='softmax')) #layer 4\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "history = model.fit(train_pad_sequences, train_encoded_labels, batch_size=batch_size, epochs=epochs, validation_data=(val_pad_sequences, val_encoded_labels))\n",
    "\n",
    "# print the train and validation accuracy\n",
    "plot_accuracy(history)\n",
    "plot_loss(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardem a results els millors resultats de validaci√≥ del experiment 3\n",
    "\n",
    "result_instance = pd.DataFrame({\n",
    "    'Experiment': ['Experiment 3'], \n",
    "    'Train Accuracy': [history.history['accuracy'][-1]], \n",
    "    'Validation Accuracy': [history.history['val_accuracy'][-1]]\n",
    "})\n",
    "\n",
    "results = pd.concat([results, result_instance], ignore_index=True)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Xarxes Recurrents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding( vocab_size,embedding_dim)) #layer 1\n",
    "\n",
    "#Bidirectional LSTM layer que retorna el embedding contextual de cada paraula (return_sequences=True).\n",
    "#Cal afegir una capa de GlobalAveragePooling1D per obtenir un embedding de tota la seq√º√®ncia.\n",
    "\n",
    "model.add(Bidirectional(LSTM(embedding_dim, return_sequences=True))) #layer 2\n",
    "model.add(GlobalAveragePooling1D()) #layer 2\n",
    "model.add(Dense(22,activation='softmax')) #layer 4\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "history = model.fit(train_pad_sequences, train_encoded_labels, batch_size=batch_size, epochs=epochs, validation_data=(val_pad_sequences, val_encoded_labels))\n",
    "\n",
    "# print the train and validation accuracy\n",
    "plot_accuracy(history)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardem a results els millors resultats de validaci√≥ del experiment 4\n",
    "\n",
    "result_instance = pd.DataFrame({\n",
    "    'Experiment': ['Experiment 4 (Pooling)'], \n",
    "    'Train Accuracy': [history.history['accuracy'][-1]], \n",
    "    'Validation Accuracy': [history.history['val_accuracy'][-1]]\n",
    "})\n",
    "\n",
    "results = pd.concat([results, result_instance], ignore_index=True)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding( vocab_size,embedding_dim)) #layer 1\n",
    "\n",
    "#Bidirectional LSTM layer que retorna el embedding contextual de tota la frase (return_sequences = False).\n",
    "model.add(Bidirectional(LSTM(embedding_dim, return_sequences=False))) #layer 2\n",
    "model.add(Dense(22,activation='softmax')) #layer 4\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "history = model.fit(train_pad_sequences, train_encoded_labels, batch_size=batch_size, epochs=epochs, validation_data=(val_pad_sequences, val_encoded_labels))\n",
    "\n",
    "# print the train and validation accuracy\n",
    "plot_accuracy(history)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardem a results els millors resultats de validaci√≥ del experiment 4.2\n",
    "\n",
    "result_instance = pd.DataFrame({\n",
    "    'Experiment': ['Experiment 4 (No Pooling)'], \n",
    "    'Train Accuracy': [history.history['accuracy'][-1]], \n",
    "    'Validation Accuracy': [history.history['val_accuracy'][-1]]\n",
    "})\n",
    "\n",
    "results = pd.concat([results, result_instance], ignore_index=True)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Regularitzaci√≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6: Balancejat de Clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJL2VFSu6sce"
   },
   "source": [
    "\n",
    "<h1><a name=\"section-four\"> 4. Lliurable </a></h1>\n",
    "\n",
    "Heu d'entregar un document PDF de com a **m√†xim 10 p√†gines** que incloga els resultats de tots els exercicis aix√≠ com una explicaci√≥ de cadascun dels resultats i de la modificaci√≥ que heu fet. L'estructura del document √©s:\n",
    "\n",
    "1. Introducci√≥.\n",
    "2. Experiments i Resultats (amb raonament).\n",
    "3. Conclusions.\n",
    "\n",
    "No cal que afegiu el vostre codi al document, podeu entregar el *notebook* juntament amb el document.\n",
    "\n",
    " ---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1Hzv2UnnX9qt2tfevnrW9wKKyxz49enYa",
     "timestamp": 1695718475771
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
